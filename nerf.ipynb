{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D49ghQcVSqg6"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!pip install --upgrade pip\n",
        "!pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 --extra-index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Installing TinyCuda\n",
        "%cd /content/\n",
        "!gdown \"https://drive.google.com/u/1/uc?id=1-7x7qQfB7bIw2zV4Lr6-yhvMpjXC84Q5&confirm=t\"\n",
        "!pip install tinycudann-1.7-cp310-cp310-linux_x86_64.whl\n",
        "\n",
        "# Install nerfstudio\n",
        "%cd /content/\n",
        "!pip install git+https://github.com/nerfstudio-project/nerfstudio.git\n",
        "\n",
        "!git clone https://github.com/alaa-khamis/looping-gifs-nerf.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run COLMAP locally using the colmap2nerf.py file in [Instant-NGP](https://github.com/NVlabs/instant-ngp)\n",
        "1. Download instant-ngp locally\n",
        "2. Follow the instructions in this file: [Instant-NGP](https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md)\n",
        "3. Upload the data to your google drive (images + transform.json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXkrvC5XTucU",
        "outputId": "4a2855bc-1a77-4f07-d471-ccf5c4e25c80"
      },
      "outputs": [],
      "source": [
        "#Load Data\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "scene = 'myvid'     #Change to directory containing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Open Viewer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y13cOeZTP0S"
      },
      "outputs": [],
      "source": [
        "\n",
        "%cd /content\n",
        "\n",
        "# Install localtunnel\n",
        "# We are using localtunnel https://github.com/localtunnel/localtunnel but ngrok could also be used\n",
        "!npm install -g localtunnel\n",
        "\n",
        "# Tunnel port 7007, the default for\n",
        "!rm url.txt 2> /dev/null\n",
        "get_ipython().system_raw('lt --port 7007 >> url.txt 2>&1 &')\n",
        "\n",
        "import time\n",
        "time.sleep(3) # the previous command needs time to write to url.txt\n",
        "\n",
        "\n",
        "with open('url.txt') as f:\n",
        "  lines = f.readlines()\n",
        "websocket_url = lines[0].split(\": \")[1].strip().replace(\"https\", \"wss\")\n",
        "# from nerfstudio.utils.io import load_from_json\n",
        "# from pathlib import Path\n",
        "# json_filename = \"nerfstudio/nerfstudio/viewer/app/package.json\"\n",
        "# version = load_from_json(Path(json_filename))[\"version\"]\n",
        "url = f\"https://viewer.nerf.studio/?websocket_url={websocket_url}\"\n",
        "print(url)\n",
        "print(\"You may need to click Refresh Page after you start training!\")\n",
        "from IPython import display\n",
        "display.IFrame(src=url, height=800, width=\"100%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train data (default model is 'nerfacto', other available models : [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf54QDrZTWKC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "%cd /content\n",
        "if os.path.exists(f\"drive/MyDrive/{scene}/transforms.json\"):\n",
        "    !ns-train nerfacto --viewer.websocket-port 7007 nerfstudio-data --data drive/MyDrive/$scene\n",
        "else:\n",
        "    from IPython.core.display import display, HTML\n",
        "    display(HTML('Error: Data processing did not complete'))\n",
        "    display(HTML('Please re-run `Downloading and Processing Data`, or view the FAQ for more info.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Camera Path Creator\n",
        "\n",
        "This Python script allows you to create a camera path from transformation data for 3D animation. The script uses JSON data as an input, containing frame and transformation data, and creates a camera path JSON file as output.\n",
        "\n",
        "## Parameters\n",
        "\n",
        "The script allows you to adjust the following parameters via command-line arguments:\n",
        "\n",
        "- `--mode`: The aspect ratio of the video. It can be 'landscape' or 'portrait'. Default value is 'landscape'.\n",
        "- `--fov`: The field of view of the camera. Default value is 50.\n",
        "- `--aspect-ratio`: The aspect ratio of the video. Default value is 1.66996699669967 (approximately 16:9), which corresponds to widescreen aspect ratio.\n",
        "- `--height`: The height of the video in pixels. Default value is 1080.\n",
        "- `--width`: The width of the video in pixels. Default value is 1920.\n",
        "- `--duration`: The duration of the video in seconds. Default value is 5.\n",
        "- `--smoothness`: Controls the smoothness of the camera path. Default value is 0.5.\n",
        "- `--data`: The path to your input JSON file with frame and transformation data. Default value is './transforms.json'.\n",
        "- `--output`: The directory where you want your output JSON file to be saved. Default value is '.' (current directory).\n",
        "\n",
        "## How to Use\n",
        "\n",
        "To run the script with default parameters, use the following command:\n",
        "\n",
        "```bash\n",
        "python camera_path_creator.py\n",
        "```\n",
        "\n",
        "To run the script with custom parameters, use the following command:\n",
        "\n",
        "```bash\n",
        "python camera_path_creator.py --mode portrait --fov 75 --aspect-ratio 1.33 --height 1080 --width 720 --duration 10 --smoothness 0.8 --data ./custom_data.json --output ./output_dir\n",
        "```\n",
        "\n",
        "This command will create a portrait video with a FOV of 75, an aspect ratio of 1.33 (4:3), a resolution of 720x1080, a duration of 10 seconds, and a smoothness value of 0.8. The input data is taken from custom_data.json and the output will be saved in the directory output_dir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transforms_path = f\"drive/MyDrive/{scene}/transforms.json\"\n",
        "output_path = \"camera_paths/\"\n",
        "\n",
        "!python create_cam_path.py --load $transforms_path --output $output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Change config_path and camera_path_file to fit your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm8xJwnSdx5e"
      },
      "outputs": [],
      "source": [
        "\n",
        "config_path = 'drive/MyDrive/nerfacto/2023-07-22_190553/config.yml'\n",
        "camera_path_file = 'camera_paths/camera_path.json'\n",
        "output_path = 'renders/output.mp4'\n",
        "\n",
        "%cd /content/\n",
        "!ns-render camera-path --load-config $config_path --camera-path-filename $camera_path_file --output-path $output_path"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
